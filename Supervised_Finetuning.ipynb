{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2609cfa5-89cb-407b-98a9-8f4a75c725aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u17a883cf852b9f81bb6e3bb6ee3b080/.conda/envs/itrex/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u17a883cf852b9f81bb6e3bb6ee3b080/.conda/envs/itrex/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 00:30:23.094391: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-29 00:30:24.075599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-29 00:30:27.477218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0cbf9a-4398-4bcf-b648-9acebc8e4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adccb110-3f4d-42af-98cd-125371663c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f1f1e8-2134-4094-9dc2-b56b425e7743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93b73f949104c34aacd6436bd018847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6720adaf55724305932fadc4e02cc95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/136k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510aa296cd794dda8dfcf5ca31e9ada4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"llamafactory/tiny-supervised-dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f043b99-919b-4bfb-bb4e-c0b6056e8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d47d4a-407b-4126-b28c-a12a7146dc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>1. Eat a balanced diet and make sure to includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>The three primary colors are red, blue, and ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            instruction input  \\\n",
       "0  Give three tips for staying healthy.         \n",
       "1    What are the three primary colors?         \n",
       "\n",
       "                                              output  \n",
       "0  1. Eat a balanced diet and make sure to includ...  \n",
       "1  The three primary colors are red, blue, and ye...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0ebb8a-71ab-48e4-b97c-a22fc98c300e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [INST] Give three tips for staying healthy. [/...\n",
       "1      [INST] What are the three primary colors? [/IN...\n",
       "2      [INST] Describe the structure of an atom. [/IN...\n",
       "3      [INST] How can we reduce air pollution? [/INST...\n",
       "4      [INST] Pretend you are a project manager of a ...\n",
       "                             ...                        \n",
       "295    [INST] 生成创新性解决方案以减少食品浪费。 [/INST] 减少食品浪费的一种可能的解...\n",
       "296    [INST] 总结下面提供的信息。 [/INST] River Technologies在2...\n",
       "297    [INST] 创建一个数据库来存储下面给出的用户数据。 [/INST] 可以创建以下数据库来...\n",
       "298    [INST] 讨论给定COVID-19疫苗对国际贸易的潜在影响。 [/INST] COVID...\n",
       "299    [INST] 提供三种方法来改善工作场所的沟通。 [/INST] 1. 鼓励开放性对话：鼓励...\n",
       "Name: Formatted, Length: 300, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to transform the row into desired format\n",
    "def format_row(row):\n",
    "    question = row['instruction']\n",
    "    answer = row['output']\n",
    "    formatted_string = f\"[INST] {question} [/INST] {answer} \"\n",
    "    return formatted_string\n",
    "\n",
    "# Apply the function to each row of the dataframe\n",
    "df['Formatted'] = df.apply(format_row, axis=1)\n",
    "\n",
    "# Display the formatted column\n",
    "df['Formatted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c896b9b-6524-452e-abbd-be4be4fce084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>1. Eat a balanced diet and make sure to includ...</td>\n",
       "      <td>[INST] Give three tips for staying healthy. [/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>The three primary colors are red, blue, and ye...</td>\n",
       "      <td>[INST] What are the three primary colors? [/IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td></td>\n",
       "      <td>An atom is made up of a nucleus, which contain...</td>\n",
       "      <td>[INST] Describe the structure of an atom. [/IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td></td>\n",
       "      <td>There are a number of ways to reduce air pollu...</td>\n",
       "      <td>[INST] How can we reduce air pollution? [/INST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretend you are a project manager of a constru...</td>\n",
       "      <td></td>\n",
       "      <td>I had to make a difficult decision when I was ...</td>\n",
       "      <td>[INST] Pretend you are a project manager of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>生成创新性解决方案以减少食品浪费。</td>\n",
       "      <td></td>\n",
       "      <td>减少食品浪费的一种可能的解决方案是创建一个在线平台，作为食品企业和家庭之间的中介。在这样的平...</td>\n",
       "      <td>[INST] 生成创新性解决方案以减少食品浪费。 [/INST] 减少食品浪费的一种可能的解...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>总结下面提供的信息。</td>\n",
       "      <td>与2017年第四季度相比，River Technologies的销售额在2018年第一季度下...</td>\n",
       "      <td>River Technologies在2018年第一季度报告销售额下降了6.2％，与2017...</td>\n",
       "      <td>[INST] 总结下面提供的信息。 [/INST] River Technologies在2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>创建一个数据库来存储下面给出的用户数据。</td>\n",
       "      <td>用户名\\n密码\\n邮箱\\n电话号码</td>\n",
       "      <td>可以创建以下数据库来存储用户数据：\\n表名：users\\n字段：username（字符串）、...</td>\n",
       "      <td>[INST] 创建一个数据库来存储下面给出的用户数据。 [/INST] 可以创建以下数据库来...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>讨论给定COVID-19疫苗对国际贸易的潜在影响。</td>\n",
       "      <td></td>\n",
       "      <td>COVID-19疫苗的开发有可能对国际贸易产生积极影响。随着各国努力为其公民接种疫苗，对基本...</td>\n",
       "      <td>[INST] 讨论给定COVID-19疫苗对国际贸易的潜在影响。 [/INST] COVID...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>提供三种方法来改善工作场所的沟通。</td>\n",
       "      <td></td>\n",
       "      <td>1. 鼓励开放性对话：鼓励开放性对话使员工能够自由地表达自己的想法，而不用担心批评或评判。这...</td>\n",
       "      <td>[INST] 提供三种方法来改善工作场所的沟通。 [/INST] 1. 鼓励开放性对话：鼓励...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  \\\n",
       "0                 Give three tips for staying healthy.   \n",
       "1                   What are the three primary colors?   \n",
       "2                   Describe the structure of an atom.   \n",
       "3                     How can we reduce air pollution?   \n",
       "4    Pretend you are a project manager of a constru...   \n",
       "..                                                 ...   \n",
       "295                                  生成创新性解决方案以减少食品浪费。   \n",
       "296                                         总结下面提供的信息。   \n",
       "297                               创建一个数据库来存储下面给出的用户数据。   \n",
       "298                          讨论给定COVID-19疫苗对国际贸易的潜在影响。   \n",
       "299                                  提供三种方法来改善工作场所的沟通。   \n",
       "\n",
       "                                                 input  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "295                                                      \n",
       "296  与2017年第四季度相比，River Technologies的销售额在2018年第一季度下...   \n",
       "297                                  用户名\\n密码\\n邮箱\\n电话号码   \n",
       "298                                                      \n",
       "299                                                      \n",
       "\n",
       "                                                output  \\\n",
       "0    1. Eat a balanced diet and make sure to includ...   \n",
       "1    The three primary colors are red, blue, and ye...   \n",
       "2    An atom is made up of a nucleus, which contain...   \n",
       "3    There are a number of ways to reduce air pollu...   \n",
       "4    I had to make a difficult decision when I was ...   \n",
       "..                                                 ...   \n",
       "295  减少食品浪费的一种可能的解决方案是创建一个在线平台，作为食品企业和家庭之间的中介。在这样的平...   \n",
       "296  River Technologies在2018年第一季度报告销售额下降了6.2％，与2017...   \n",
       "297  可以创建以下数据库来存储用户数据：\\n表名：users\\n字段：username（字符串）、...   \n",
       "298  COVID-19疫苗的开发有可能对国际贸易产生积极影响。随着各国努力为其公民接种疫苗，对基本...   \n",
       "299  1. 鼓励开放性对话：鼓励开放性对话使员工能够自由地表达自己的想法，而不用担心批评或评判。这...   \n",
       "\n",
       "                                                  Text  \n",
       "0    [INST] Give three tips for staying healthy. [/...  \n",
       "1    [INST] What are the three primary colors? [/IN...  \n",
       "2    [INST] Describe the structure of an atom. [/IN...  \n",
       "3    [INST] How can we reduce air pollution? [/INST...  \n",
       "4    [INST] Pretend you are a project manager of a ...  \n",
       "..                                                 ...  \n",
       "295  [INST] 生成创新性解决方案以减少食品浪费。 [/INST] 减少食品浪费的一种可能的解...  \n",
       "296  [INST] 总结下面提供的信息。 [/INST] River Technologies在2...  \n",
       "297  [INST] 创建一个数据库来存储下面给出的用户数据。 [/INST] 可以创建以下数据库来...  \n",
       "298  [INST] 讨论给定COVID-19疫苗对国际贸易的潜在影响。 [/INST] COVID...  \n",
       "299  [INST] 提供三种方法来改善工作场所的沟通。 [/INST] 1. 鼓励开放性对话：鼓励...  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the 'Formatted' column to 'Text'\n",
    "new_df = df.rename(columns={'Formatted': 'Text'})\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f527c6-3d47-491a-abcf-723b57883a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] Give three tips for staying healthy. [/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] What are the three primary colors? [/IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] Describe the structure of an atom. [/IN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  [INST] Give three tips for staying healthy. [/...\n",
       "1  [INST] What are the three primary colors? [/IN...\n",
       "2  [INST] Describe the structure of an atom. [/IN..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df[['Text']]\n",
    "new_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a95fc1-f703-4ca8-b205-1722e1b2742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('formatted_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a080bf-eb37-47ca-98bf-ff6a4f902ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"formatted_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ff4244-94a3-4634-804a-4bebbc94df24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] Give three tips for staying healthy. [/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] What are the three primary colors? [/IN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  [INST] Give three tips for staying healthy. [/...\n",
       "1  [INST] What are the three primary colors? [/IN..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72306847-1121-41ed-81d8-ef1b5556f79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33422154dfbb445f8bbcc92ed4a7d312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_dataset = load_dataset(\"csv\", data_files=\"formatted_data.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca7e5edf-ea66-4280-9f62-9b609518259f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5ddc672-139e-41bc-8cab-cce23a631bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u17a883cf852b9f81bb6e3bb6ee3b080/.conda/envs/itrex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2e5450e43f4264803d94e0a49f7dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u17a883cf852b9f81bb6e3bb6ee3b080/.conda/envs/itrex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b262f04ad464ffea726be25e6b5a337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "new_model = \"Llama-2-mental-health\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir='./tmp',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,  # Increased batch size\n",
    "    per_device_eval_batch_size=8,   # Increased batch size\n",
    "    gradient_accumulation_steps=1,  # Reduced gradient accumulation steps\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    no_cuda=True  # Disable CUDA (GPU) usage\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=training_dataset,  # Training datadet being used here\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=500,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d6f2ec-6b3c-4bf4-b7d8-ae21c49df0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u17a883cf852b9f81bb6e3bb6ee3b080/.conda/envs/itrex/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 114\n",
      "  Number of trainable parameters = 33,554,432\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 21:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=1.486392104834841, metrics={'train_runtime': 1313.9695, 'train_samples_per_second': 0.685, 'train_steps_per_second': 0.087, 'total_flos': 1.1887313760288768e+16, 'train_loss': 1.486392104834841, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd4e9e58-ede3-481e-9686-2b523eec739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1081fb0-e96a-4092-85f8-3f4ad7a00da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/tokenizer.json\n",
      "loading file added_tokens.json from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"NousResearch/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f928bb9e20645868d847565684a73e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at NousResearch/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/u17a883cf852b9f81bb6e3bb6ee3b080/.cache/huggingface/hub/models--NousResearch--Llama-2-7b-chat-hf/snapshots/351844e75ed0bcbbe3f10671b3c808d2b83894ee/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_p\": 0.6,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] How can we reduce air pollution? [/INST]  Reducing air pollution requires a multi-faceted approach that involves individual actions, government policies, and technological innovations. Here are some ways to reduce air pollution:\n",
      "\n",
      "1. Use public transportation, walk, or bike: Using public transportation, walking, or biking instead of driving can significantly reduce air pollution caused by vehicle emissions.\n",
      "2. Use energy-efficient appliances: Using energy-efficient appliances can reduce energy consumption, which can lead to lower emissions.\n",
      "3. Reduce, reuse, recycle: Reduce consumption, reuse products, and recycle materials to reduce waste and lower emissions.\n",
      "4. Plant trees and greenery: Trees and other greenery help absorb pollutants from the air, reducing air pollution.\n",
      "5. Use clean energy sources: Transitioning to clean energy sources like solar, wind, and hydroelectric power can reduce emissions from fossil fuels.\n",
      "6. Implement emissions controls: Governments can implement emissions controls, such as fuel standards, emissions limits, and pollution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['USE_IPEX'] = '0'\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "prompt = \"How can we reduce air pollution?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=250)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37f1aa0-968a-4c80-b79b-460cef250067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 07:45:36.233663: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-29 07:45:37.230579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-29 07:45:40.591267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/u17a883cf852b9f81bb6e3bb6ee3b080/.conda/envs/itrex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291de93902d9468bb49a087b9890bb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u17a883cf852b9f81bb6e3bb6ee3b080/.conda/envs/itrex/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a poem that expresses anger. [/INST]  Sure, here's a poem that expresses anger:\n",
      "\n",
      "Raging fire in my veins,\n",
      "A tempest raging out of bounds,\n",
      "Fury in my heart, a stormy sea,\n",
      "Unleashed, untamed, and wild as can be.\n",
      "\n",
      "I feel the heat, the pressure builds,\n",
      "My soul is consumed by bitter pills,\n",
      "The injustice, the betrayal, the pain,\n",
      "A never-ending cycle, a never-ending strain.\n",
      "\n",
      "I'm trapped in this cage of rage,\n",
      "A prisoner of my own disgrace,\n",
      "The world outside, a blurred haze,\n",
      "As I struggle to find my way to peace.\n",
      "\n",
      "But peace is hard to find,\n",
      "When anger's all that's on my mind,\n",
      "A never-ending battle, a war within,\n",
      "A fight to keep my sanity in check, to begin.\n",
      "\n",
      "So I'll let the anger flow,\n",
      "Like a river that won't let go,\n",
      "For in its depths,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['USE_IPEX'] = '0'\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "prompt = \"Generate a poem that expresses anger.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=250)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc509b-734f-45ef-8ddd-b5b710029b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-chat-3",
   "language": "python",
   "name": "neural-chat-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
